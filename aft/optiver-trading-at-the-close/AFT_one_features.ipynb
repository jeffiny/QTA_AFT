{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":57891,"databundleVersionId":7056235,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-26T05:23:17.915422Z","iopub.execute_input":"2024-10-26T05:23:17.915933Z","iopub.status.idle":"2024-10-26T05:23:19.137380Z","shell.execute_reply.started":"2024-10-26T05:23:17.915887Z","shell.execute_reply":"2024-10-26T05:23:19.135797Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/optiver-trading-at-the-close/public_timeseries_testing_util.py\n/kaggle/input/optiver-trading-at-the-close/train.csv\n/kaggle/input/optiver-trading-at-the-close/example_test_files/sample_submission.csv\n/kaggle/input/optiver-trading-at-the-close/example_test_files/revealed_targets.csv\n/kaggle/input/optiver-trading-at-the-close/example_test_files/test.csv\n/kaggle/input/optiver-trading-at-the-close/optiver2023/competition.cpython-310-x86_64-linux-gnu.so\n/kaggle/input/optiver-trading-at-the-close/optiver2023/__init__.py\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport polars as pl\nfrom tqdm import tqdm\nimport itertools\nfrom itertools import combinations\nimport warnings\nfrom numba import njit, prange\nimport time\nfrom functools import wraps\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-10-26T05:27:16.026734Z","iopub.execute_input":"2024-10-26T05:27:16.027332Z","iopub.status.idle":"2024-10-26T05:27:18.797778Z","shell.execute_reply.started":"2024-10-26T05:27:16.027282Z","shell.execute_reply":"2024-10-26T05:27:18.796430Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def time_it(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        start_time = time.time()  # 记录开始时间\n        result = func(*args, **kwargs)  # 调用被装饰的函数\n        end_time = time.time()  # 记录结束时间\n        execution_time = end_time - start_time  # 计算执行时间\n        print(f\"Function '{func.__name__}' executed in {execution_time:.4f} seconds\")\n        return result  # 返回函数结果\n    return wrapper","metadata":{"execution":{"iopub.status.busy":"2024-10-26T05:27:18.800372Z","iopub.execute_input":"2024-10-26T05:27:18.801586Z","iopub.status.idle":"2024-10-26T05:27:18.809838Z","shell.execute_reply.started":"2024-10-26T05:27:18.801522Z","shell.execute_reply":"2024-10-26T05:27:18.808085Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"ORIGINAL_TRAIN = pd.read_csv('/kaggle/input/optiver-trading-at-the-close/train.csv',engine = 'pyarrow')","metadata":{"execution":{"iopub.status.busy":"2024-10-26T05:27:18.811753Z","iopub.execute_input":"2024-10-26T05:27:18.812333Z","iopub.status.idle":"2024-10-26T05:27:27.648251Z","shell.execute_reply.started":"2024-10-26T05:27:18.812270Z","shell.execute_reply":"2024-10-26T05:27:27.646913Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# 1. 市场相关features ——zzc","metadata":{}},{"cell_type":"code","source":"def market_features(df_1day):\n    weights = [\n        0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n        0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n        0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n        0.004, 0.004, 0.006, 0.002, 0.002, 0.04, 0.002, 0.002, 0.004, 0.04, 0.002, 0.001,\n        0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n        0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n        0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n        0.02, 0.004, 0.006, 0.002, 0.02, 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n        0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n        0.004, 0.006, 0.006, 0.001, 0.04, 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n        0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n        0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n        0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n        0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n        0.04, 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02, 0.004, 0.002, 0.006, 0.02,\n        0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n        0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n    ]\n    weights = pd.Series(weights)\n    weights.index.name = 'stock_id'\n\n    market_feats = pd.DataFrame(\n        index=df_1day.index.get_level_values(0).unique())\n    wap = df_1day['wap'].unstack()\n    market_feats['index_wap_std'] = wap.std(axis=1)\n    stock_ret1 = 10000*(wap/wap.shift(1)-1)\n    market_feats['index_ret1'] = stock_ret1.mean(axis=1)\n    market_feats['index_ret1_std'] = stock_ret1.std(axis=1)\n    market_feats['index_ret1_w'] = stock_ret1.mul(weights, axis=1).sum(axis=1)\n    stock_ret6 = 10000*(wap/wap.shift(6)-1)\n    market_feats['index_ret6'] = stock_ret6.mean(axis=1)\n    market_feats['index_ret6_std'] = stock_ret6.std(axis=1)\n    market_feats['index_ret6_w'] = stock_ret6.mul(weights, axis=1).sum(axis=1)\n    ask_size = df_1day['ask_size'].unstack()\n    bid_size = df_1day['bid_size'].unstack()\n    market_feats['market_volume'] = ask_size.mean(axis=1)+bid_size.mean(axis=1)\n    ask_price = df_1day['ask_price'].unstack()\n    bid_price = df_1day['bid_price'].unstack()\n    market_feats['market_spread'] = ask_price.mean(\n        axis=1)-bid_price.mean(axis=1)\n    market_feats['market_max_spread'] = ask_price.max(\n        axis=1)-bid_price.min(axis=1)\n    market_feats['mfd'] = market_feats.index // 60\n    \n    return market_feats\n@time_it\ndef cal_market_features(df):\n    feature_0 = df.set_index(['seconds_in_bucket', 'stock_id'])\n    feature_market = feature_0.groupby('date_id').apply(market_features)\n    feature_market = feature_0.reset_index()[['stock_id', 'date_id', 'seconds_in_bucket']].merge(right=feature_market, on=['date_id','seconds_in_bucket'], how='left')\n    #feature_market.insert(2, 'seconds_in_bucket', feature_market.pop('seconds_in_bucket'))\n\n    return feature_market\nmarket_features = cal_market_features(ORIGINAL_TRAIN.copy())\nmarket_features = pl.DataFrame(market_features)","metadata":{"execution":{"iopub.status.busy":"2024-10-26T04:31:43.576624Z","iopub.execute_input":"2024-10-26T04:31:43.576999Z","iopub.status.idle":"2024-10-26T04:32:06.041225Z","shell.execute_reply.started":"2024-10-26T04:31:43.576958Z","shell.execute_reply":"2024-10-26T04:32:06.040026Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Function 'cal_market_features' executed in 21.4622 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"market_features.write_parquet(\"market_features.parquet\")","metadata":{"execution":{"iopub.status.busy":"2024-10-26T04:34:20.449961Z","iopub.execute_input":"2024-10-26T04:34:20.450808Z","iopub.status.idle":"2024-10-26T04:34:21.131971Z","shell.execute_reply.started":"2024-10-26T04:34:20.450752Z","shell.execute_reply":"2024-10-26T04:34:21.130656Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# 2. 截面因子——zzh","metadata":{}},{"cell_type":"code","source":"@time_it\ndef cross_section_features_from_word(df):\n    # 第一批特征：2个价格的差、乘、imblance，3个价格的最大、最小、中位数、imbalance\n    prices = ['reference_price','far_price', 'near_price', 'ask_price', 'bid_price', 'wap']\n    for c in itertools.combinations(prices, 2):\n        df[f'{c[0]}_minus_{c[1]}'] = (df[f'{c[0]}'] - df[f'{c[1]}']).astype(np.float32)\n        df[f'{c[0]}_times_{c[1]}'] = (df[f'{c[0]}'] * df[f'{c[1]}']).astype(np.float32)\n        df[f'{c[0]}_{c[1]}_imb'] = df.eval(f'({c[0]}-{c[1]})/({c[0]}+{c[1]})')\n\n    for c in itertools.combinations(prices, 3):\n        max_ = df[list(c)].max(axis=1)\n        min_ = df[list(c)].min(axis=1)\n        mid_ = df[list(c)].sum(axis=1)- min_ - max_\n        df[f'{c[0]}_{c[1]}_{c[2]}_imb2'] = (max_ - mid_) / (mid_ - min_)\n\n    df['mid_price'] = ((df['ask_price'] + df['bid_price']) / 2).astype(np.float32)\n    df['wap_2'] = ((df['bid_price'] * df['bid_size'] + df['ask_price'] * df['ask_size']) / (df['ask_price'] + df['bid_price'])).astype(np.float32)\n    df['best_bid_size'] = (df['bid_price'] * df['bid_size']).astype(np.float32)\n    df['best_ask_size'] = (df['ask_price'] * df['ask_size']).astype(np.float32)\n    df['bid_price_over_ask_price'] = (df['bid_price'] / df['ask_price']).astype(np.float32)\n    df['best_bid_and_ask_size'] = (df['best_bid_size'] + df['best_ask_size']) / 2\n    df['BBO_size_imbalance'] = (df['best_bid_size'] - df['best_ask_size']) / (df['best_bid_size'] + df['best_ask_size'])\n    df['effective_spread'] = 2 * np.abs(df['wap'] - 0.5 * (df.ask_price + df.bid_price)) / (0.5 * (df.ask_price + df.bid_price))\n    df['effective_spread'] = df['effective_spread'].astype(np.float32)\n    df['market_depth_vol'] = (df.bid_size + df.ask_size).astype(np.float32)\n    df['ask_bid_size_diff'] = (df.ask_size - df.bid_size).astype(np.float32)\n    df['bid_size_over_ask_size'] = (df.bid_size / df.ask_size).astype(np.float32)\n    df['shallowLIX'] = np.log10(df.matched_size / (df.ask_size + df.bid_size)).astype(np.float32)\n    df['imbalance_ratio'] = (df.imbalance_size / (df.matched_size + df.imbalance_size)).astype(np.float32)\n    df['imb_s1'] = ((df.bid_size - df.ask_size) / (df.bid_size + df.ask_size)).astype(np.float32)\n    df['imb_s2'] = ((df.imbalance_size - df.matched_size) / (df.imbalance_size + df.matched_size)).astype(np.float32)\n    median_vol = df.groupby(\"stock_id\")['bid_size'].median() + df.groupby(\"stock_id\")['ask_size'].median()\n    df['median_vol'] = df['stock_id'].map(median_vol.to_dict()).astype(np.float32)\n    df['high_volume'] = np.where(df['bid_price'] + df['ask_price'] > df['median_vol'], 1, 0).astype(np.float32)\n    df['high_volume_imbalance_size'] = (df['high_volume'] * df['imbalance_size']).astype(np.float32)\n    df['low_volume_matched_size'] = (df['high_volume'] * df['matched_size']).astype(np.float32)\n    # 一些关于volume的特征\n    max_vol = df.groupby(\"stock_id\")[['bid_size', 'ask_size']].max().sum(axis=1)\n    df['max_vol'] = df['stock_id'].map(max_vol.to_dict()).astype(np.float32)\n    min_vol = df.groupby(\"stock_id\")[['bid_size', 'ask_size']].min().sum(axis=1)\n    df['min_vol'] = df['stock_id'].map(min_vol.to_dict()).astype(np.float32)\n    std_vol = df.groupby(\"stock_id\")[['bid_size', 'ask_size']].std().sum(axis=1)\n    df['std_vol'] = df['stock_id'].map(std_vol.to_dict()).astype(np.float32)\n\n    first_5min_volume = df.loc[(df.far_price.isna()) | (df.near_price.isna())].groupby('stock_id')[['bid_size', 'ask_size']].median().sum(axis=1)\n    df['first_5min_volume'] = df['stock_id'].map(first_5min_volume.to_dict()).astype(np.float32)\n    last_5min_volume = df[[\"stock_id\",'bid_size','ask_size','near_price','far_price']].dropna().groupby('stock_id')[['bid_size','ask_size']].median().sum(axis=1)\n    df['last_5min_volume'] = df['stock_id'].map(last_5min_volume.to_dict()).astype(np.float32)\n    df['first_5min'] = np.where((df.far_price.isna()) | (df.near_price.isna()), 1, 0).astype(np.float32)\n    df['last_5min'] = np.where((~df.far_price.isna()) & (~df.near_price.isna()) & (~df.stock_id.isna()) & (~df.bid_size.isna()) & (~df.ask_size.isna()), 1, 0).astype(np.float32)\n    df['LogquoteSlope'] = ((np.log10(df.ask_price) - np.log10(df.bid_price)) / (np.log10(df.ask_price) + np.log10(df.bid_price))).astype(np.float32)\n    middle_price = (df.ask_price + df.bid_price) / 2\n    vwap_a = df.groupby(\"stock_id\")[['ask_price', 'ask_size']].apply(lambda x: (x['ask_price'] * x['ask_size']).sum()) / df.groupby(\"stock_id\")['ask_size'].sum()\n    dolvol_a = df['stock_id'].map(df.groupby(\"stock_id\")[['ask_price', 'ask_size']].apply(lambda x: (x['ask_price'] * x['ask_size']).sum()).to_dict()).astype(np.float32)\n    df['vwap_a'] = df['stock_id'].map(vwap_a.to_dict()).astype(np.float32)\n    vwap_b = df.groupby(\"stock_id\")[['bid_price', 'bid_size']].apply(lambda x: (x['bid_price'] * x['bid_size']).sum()) / df.groupby(\"stock_id\")['bid_size'].sum()\n    dolvol_b = df['stock_id'].map(df.groupby(\"stock_id\")[['bid_price', 'bid_size']].apply(lambda x: (x['bid_price'] * x['bid_size']).sum()).to_dict()).astype(np.float32)\n    df['vwap_b'] = df['stock_id'].map(vwap_b.to_dict()).astype(np.float32)\n    df['vwapm_a'] = ((df['vwap_a'] - middle_price) / middle_price).astype(np.float32)\n    df['vwapm_b'] = ((df['vwap_b'] - middle_price) / middle_price).astype(np.float32)\n    df['MCI_a'] = (df['vwapm_a'] / dolvol_a).astype(np.float32)\n    df['MCI_b'] = (df['vwapm_b'] / dolvol_b).astype(np.float32)\n    # imbalance_buy_sell_flag的one-hot特征\n    df['imbalance_buy_flag'] = df['imbalance_buy_sell_flag'].where(df['imbalance_buy_sell_flag']==1,0).astype(np.float32)\n    df['imbalance_sell_flag'] = -df['imbalance_buy_sell_flag'].where(df['imbalance_buy_sell_flag']==-1,0).astype(np.float32)\n    last_5_min_match_size_sum = df.loc[df.seconds_in_bucket >= 300].groupby([\"stock_id\", \"date_id\"])['matched_size'].sum()\n    all_match_size_sum = df.groupby([\"stock_id\", \"date_id\"])['matched_size'].sum()\n    ratio = last_5_min_match_size_sum / all_match_size_sum\n    ratio\n    # 集合竞价成交占比因子：最后五分钟的match_size的和与全部十分钟的match_size的和的比值在过去N天内的平均\n    last_5_min_match_size_sum = df.loc[df.seconds_in_bucket >= 300].groupby([\"stock_id\", \"date_id\"])['matched_size'].sum()\n    all_match_size_sum = df.groupby([\"stock_id\", \"date_id\"])['matched_size'].sum()\n    ratio = last_5_min_match_size_sum / all_match_size_sum\n    for n in tqdm(range(3, 10, 2)):\n        factor = ratio.groupby(level=0).transform(lambda x: x.rolling(n, min_periods=1).mean())\n        factor.name = f\"CMV_{n}\"\n        factor = factor.reset_index(drop=False)\n        df = pd.merge(left=df, right=factor, on=['stock_id', 'date_id'], how='left')\n    # 尾盘信息因子：利用bid_size, ask_size, market_depth_vol, best_bid_size, best_ask_size做同样的逻辑处理\n    features = [\"bid_size\", \"ask_size\", \"market_depth_vol\", \"best_bid_size\", \"best_ask_size\"]\n    for feature in tqdm(features):\n        last_5_min_size_sum = df.loc[df.seconds_in_bucket >= 300].groupby([\"stock_id\", \"date_id\"])[feature].sum()\n        all_size_sum = df.groupby([\"stock_id\", \"date_id\"])[feature].sum()\n        ratio = last_5_min_size_sum / all_size_sum\n        for n in range(3, 10, 2):\n            factor = ratio.groupby(level=0).transform(lambda x: x.rolling(n, min_periods=1).mean())\n            factor.name = f\"CMV_{n}_{feature}\"\n            factor = factor.reset_index(drop=False)\n            df = pd.merge(left=df, right=factor, on=['stock_id', 'date_id'], how='left')\n    relative_to_max = df.groupby([\"stock_id\", \"date_id\"])['reference_price'].apply(lambda x: x.tail(1).values[0] / x.max() - 1).rename(\"relative_to_max\").reset_index(drop=False)\n    df = pd.merge(left=df, right=relative_to_max, on=['stock_id', 'date_id'], how='left')\n    relative_to_min = df.groupby([\"stock_id\", \"date_id\"])['reference_price'].apply(lambda x: x.tail(1).values[0] / x.min() - 1).rename(\"relative_to_min\").reset_index(drop=False)\n    df = pd.merge(left=df, right=relative_to_min, on=['stock_id', 'date_id'], how='left')\n    # ZXY: 和第一部分重复了\n    # 流动性\n    df['Liquidity_1'] = (df.far_price / df.wap - 1) / df.matched_size\n    df['Liquidity_2'] = (df.far_price / df.mid_price - 1) / df.matched_size\n    df['Liquidity_3'] = (df.far_price / df.near_price - 1) / df.matched_size\n    df['Liquidity_4'] = (df.far_price / df.reference_price - 1) / df.matched_size\n    df['Liquidity_5'] = (df[[\"reference_price\", \"far_price\", \"near_price\", \"wap\"]].max(axis=1) / df[[\"reference_price\", \"far_price\", \"near_price\", \"wap\"]].min(axis=1) - 1) / df.matched_size\n    # 收益趋势相关\n    # 各个价格和时间的相关性\n    for feature in ['mid_price', 'far_price', 'near_price', 'reference_price']:\n        factor = df.groupby([\"stock_id\", \"date_id\"])[[feature, 'seconds_in_bucket']].apply(lambda x: x[feature].corr(x['seconds_in_bucket']))\n        factor.name = f\"{feature}_corr\"\n        df = pd.merge(left=df, right=factor.reset_index(drop=False), on=['stock_id', 'date_id'], how='left').fillna(0.0)\n\n    # cumprod(FarPrice/(BidPrice_(t) + AskPrice_(t )))\n    tmp = df[['seconds_in_bucket', 'stock_id', \"date_id\"]].copy()\n    tmp['far_mid_val'] = df['far_price'] / df['mid_price']\n    tmp = tmp.set_index([\"stock_id\", \"date_id\", \"seconds_in_bucket\"]).groupby(level=[0, 1]).cumprod()\n    df = df.merge(tmp.reset_index(drop=False), on=[\"stock_id\", \"date_id\", \"seconds_in_bucket\"], how='left')\n    # cumprod(wap_t/wap_(t-1))\n    wap_trend = df[[\"stock_id\", \"date_id\", \"seconds_in_bucket\", \"wap\"]].copy().set_index([\"stock_id\", \"date_id\", \"seconds_in_bucket\"])\n    wap_trend = wap_trend.groupby(level=[0, 1]).pct_change(fill_method=None)\n    wap_trend = (wap_trend + 1).groupby(level=[0, 1]).cumprod()\n    df = pd.merge(left=df, right=wap_trend.reset_index(drop=False), on=['stock_id', 'date_id', 'seconds_in_bucket'], how='left')\n    mid_trend = df[[\"stock_id\", \"date_id\", \"seconds_in_bucket\", \"mid_price\"]].copy().set_index([\"stock_id\", \"date_id\", \"seconds_in_bucket\"])\n    mid_trend = mid_trend.groupby(level=[0, 1]).pct_change(fill_method=None)\n    mid_trend = (mid_trend + 1).groupby(level=[0, 1]).cumprod()\n    df = pd.merge(left=df, right=mid_trend.reset_index(drop=False), on=['stock_id', 'date_id', 'seconds_in_bucket'], how='left')\n    return df\ncross_section_features = cross_section_features_from_word(ORIGINAL_TRAIN.copy())\n","metadata":{"execution":{"iopub.status.busy":"2024-10-26T04:34:40.429080Z","iopub.execute_input":"2024-10-26T04:34:40.429865Z","iopub.status.idle":"2024-10-26T04:43:09.063546Z","shell.execute_reply.started":"2024-10-26T04:34:40.429816Z","shell.execute_reply":"2024-10-26T04:43:09.062152Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"100%|██████████| 4/4 [00:18<00:00,  4.62s/it]\n100%|██████████| 5/5 [01:32<00:00, 18.55s/it]\n","output_type":"stream"},{"name":"stdout","text":"Function 'cross_section_features_from_word' executed in 508.0158 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"cross_section_features = pl.DataFrame(cross_section_features)\ncross_section_features.write_parquet(\"cross_section_features.parquet\")","metadata":{"execution":{"iopub.status.busy":"2024-10-26T04:43:09.066375Z","iopub.execute_input":"2024-10-26T04:43:09.066940Z","iopub.status.idle":"2024-10-26T04:43:59.206363Z","shell.execute_reply.started":"2024-10-26T04:43:09.066874Z","shell.execute_reply":"2024-10-26T04:43:59.204823Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# 3. 时序因子——djh","metadata":{}},{"cell_type":"code","source":"@time_it\ndef timeseris_features_from_word(df):\n    df['matched_percent'] = df['matched_size'] / (df['matched_size'] + df['imbalance_size'])\n\n    # 个股买卖强弱（时序）\n    is_buy = df['imbalance_buy_sell_flag'] == 1\n    is_sell = df['imbalance_buy_sell_flag'] == -1\n    is_flat = df['imbalance_buy_sell_flag'] == 0\n\n    # Using rolling windows and summing the boolean values directly\n    df['buy_count'] = is_buy.rolling(window=11).sum().shift(-10)\n    df['sell_count'] = is_sell.rolling(window=11).sum().shift(-10)\n    df['flat_count'] = is_flat.rolling(window=11).sum().shift(-10)\n    df['buy_ratio'] = df['buy_count'] / 11\n    df['sell_ratio'] = df['sell_count'] / 11\n    df['flat_ratio'] = df['flat_count'] / 11\n\n    # 不平衡方向的 imbalance_size\n    df['buy_sell_imbalance_size'] = df['imbalance_size'] * df['imbalance_buy_sell_flag']\n\n    # flag 方向发生变化的时间点\n    df['imb_change'] = df['buy_sell_imbalance_size'].diff()\n    unique_stock_count = (df.groupby(['date_id', 'seconds_in_bucket', 'imbalance_buy_sell_flag'])['stock_id']\n                      .nunique()\n                      .unstack(fill_value=0)\n                      .reset_index()\n                      .rename(columns={1: 'unique_buy_stock_count', \n                                       0: 'unique_flat_stock_count', \n                                      -1: 'unique_sell_stock_count'}))\n\n    df = df.merge(unique_stock_count, on=['date_id', 'seconds_in_bucket'], how='left')\n\n    # 添加买卖强弱与市场作比较（横截面排序）\n    df['buy_imb_degree_rank'] = df['buy_count'].rank()\n    df['sell_imb_degree_rank'] = df['sell_count'].rank()\n    df['flat_imb_degree_rank'] = df['flat_count'].rank()\n    # 不同时间段统计特征构建的因子\n    df['time_bucket'] = df['seconds_in_bucket'] // 110\n\n    aggregations = {\n        'reference_price': ['mean', 'std', 'median'],\n        'bid_price': ['mean', 'std', 'median'],\n        'ask_price': ['mean', 'std', 'median'],\n        'imbalance_size': ['mean', 'std', 'median'],\n        'matched_size': ['mean', 'std', 'median'],\n        'ask_size': ['mean', 'std', 'median'],\n        'bid_size': ['mean', 'std', 'median']\n    }\n\n    # Compute aggregations directly for mean, std, and median\n    agg_df = df.groupby(['stock_id', 'date_id', 'time_bucket']).agg(aggregations).reset_index()\n\n    # Flatten MultiIndex columns\n    agg_df.columns = ['_'.join(col).strip() if col[1] else col[0] for col in agg_df.columns]\n\n    # Custom functions for skew and kurtosis (applied selectively on required columns)\n    for col in ['reference_price', 'bid_price', 'ask_price', 'imbalance_size', 'matched_size', 'ask_size', 'bid_size']:\n        agg_df[f'skew_{col}'] = df.groupby(['stock_id', 'date_id', 'time_bucket'])[col].skew().values\n    #     agg_df[f'kurt_{col}'] = df.groupby(['stock_id', 'date_id', 'time_bucket'])[col].kurt().values\n    matched_size_quantiles = (\n        df.groupby(['date_id'])['matched_size']\n        .quantile([0.5, 0.75, 0.90])\n        .unstack(level=-1)\n        .rename(columns={0.5: 'median_matched_size', 0.75: 'q75_matched_size', 0.90: 'q90_matched_size'})\n        .reset_index()\n    )\n\n    # 将分位数与原始数据合并\n    df = df.merge(matched_size_quantiles, on='date_id', how='left')\n\n    # 根据 matched_size 对订单大小分类\n    conditions = [\n        (df['matched_size'] <= df['median_matched_size']),\n        (df['matched_size'] > df['median_matched_size']) & (df['matched_size'] <= df['q75_matched_size']),\n        (df['matched_size'] > df['q75_matched_size'])\n    ]\n    choices = ['small', 'medium', 'large']\n    df['order_size_category'] = np.select(conditions, choices)\n\n    # 计算分组统计特征\n    order_stats = (\n        df.groupby(['order_size_category', 'date_id'])\n        .agg(\n            os_order_count=('matched_size', 'count'),\n            os_mean_reference_price=('reference_price', 'mean'),\n            os_std_reference_price=('reference_price', 'std'),\n            os_median_reference_price=('reference_price', 'median'),\n            os_skew_reference_price=('reference_price', 'skew'),\n\n            os_mean_bid_price=('bid_price', 'mean'),\n            os_std_bid_price=('bid_price', 'std'),\n            os_median_bid_price=('bid_price', 'median'),\n            os_skew_bid_price=('bid_price', 'skew'),\n\n            os_mean_ask_price=('ask_price', 'mean'),\n            os_std_ask_price=('ask_price', 'std'),\n            os_median_ask_price=('ask_price', 'median'),\n            os_skew_ask_price=('ask_price','skew')\n        )\n    ).reset_index()\n\n    # 将统计特征与原始数据合并\n    df = df.merge(order_stats, on=['order_size_category', 'date_id'], how='left')\n    # 计算bid_size和ask_size的滚动标准差\n    df['bid_size_std_30_rolling'] = df['bid_size'].rolling(window=30, min_periods=1).std()\n    df['bid_size_std_10_rolling'] = df['bid_size'].rolling(window=10, min_periods=1).std()\n    df['bid_size_std_5_rolling'] = df['bid_size'].rolling(window=5, min_periods=1).std()\n\n    df['ask_size_std_30_rolling'] = df['ask_size'].rolling(window=30, min_periods=1).std()\n    df['ask_size_std_10_rolling'] = df['ask_size'].rolling(window=10, min_periods=1).std()\n    df['ask_size_std_5_rolling'] = df['ask_size'].rolling(window=5, min_periods=1).std()\n\n    # 计算reference_price的收益率\n    df['reference_price_ret'] = df['reference_price'].pct_change(fill_method=None)\n    df['matched_size_ret'] = df['matched_size'].pct_change(fill_method=None)\n    df['far_price_ret'] = df['far_price'].pct_change(fill_method=None)\n    df['near_price_ret'] = df['near_price'].pct_change(fill_method=None)\n    df['bid_price_ret'] = df['bid_price'].pct_change(fill_method=None)\n    df['ask_price_ret'] = df['ask_price'].pct_change(fill_method=None)\n    df['wap_ret'] = df['wap'].pct_change(fill_method=None)\n\n    # 计算前一分钟收益率的标准差和偏度\n    df['reference_price_ret_std_60'] = df.groupby('stock_id')['reference_price'].rolling(window=6, min_periods=1).std().reset_index(level=0, drop=True)\n    df['reference_price_skew_60'] = df.groupby('stock_id')['reference_price'].rolling(window=6, min_periods=1).skew().reset_index(level=0, drop=True)\n\n    # 将收益分成涨跌\n    df['gain'] = df['wap_ret'].apply(lambda x: x if x > 0 else 0)\n    df['loss'] = df['wap_ret'].apply(lambda x: -x if x < 0 else 0)\n    # 计算 RSI（14日周期）\n    df['avg_gain'] = df.groupby('stock_id')['gain'].rolling(window=11, min_periods=1).mean().reset_index(level=0, drop=True)\n    df['avg_loss'] = df.groupby('stock_id')['loss'].rolling(window=11, min_periods=1).mean().reset_index(level=0, drop=True)\n    df['rs'] = df['avg_gain'] / df['avg_loss']\n    df['rsi_14'] = 100 - (100 / (1 + df['rs']))\n    \n    # 计算 reference_price 和 matched_size 的相关系数\n    prices = ['reference_price', 'bid_price', 'ask_price', 'wap', 'far_price','near_price']\n    sizes = ['matched_size', 'imbalance_size', 'ask_size','bid_size']\n    for price in prices:\n        for size in sizes:\n\n            if price in df.columns and size in df.columns:\n                # 计算滚动相关性，并赋值给新列\n                df[f'{price}_{size}_corr_60'] = df.groupby('stock_id').apply(lambda x: x[price].rolling(window=6, min_periods=1).corr(x[size]),include_groups=False).reset_index(level=0, drop=True)\n                # 计算1分钟和2分钟的滚动总和\n                rolling_size_sum_1min = df[size].rolling(window=5, min_periods=1).sum()\n                rolling_size_sum_2min = df[size].rolling(window=11, min_periods=1).sum()\n\n                # 计算 VWAP 1分钟和2分钟\n                price_sum_1min = df[price].rolling(window=5, min_periods=1).sum()\n                price_sum_2min = df[price].rolling(window=11, min_periods=1).sum()\n\n                df[f'{price}_vwap_1min'] = price_sum_1min / rolling_size_sum_1min\n                df[f'{price}_vwap_2min'] = price_sum_2min / rolling_size_sum_2min\n\n                # 计算等权均值 1 分钟和 2 分钟\n                df[f'{price}_simple_mean_1min'] = df[price].rolling(window=5, min_periods=1).mean()\n                df[f'{price}_simple_mean_2min'] = df[price].rolling(window=11, min_periods=1).mean()\n\n                # 计算 VWAP 和等权均值的差值\n                df[f'{price}_vwap_minus_simple_1min'] = df[f'{price}_vwap_1min'] - df[f'{price}_simple_mean_1min']\n                df[f'{price}_vwap_minus_simple_2min'] = df[f'{price}_vwap_2min'] - df[f'{price}_simple_mean_2min']\n                \n    # 计算 bid_size 和 ask_size 的相关系数 rho\n    window_size = 5\n    df['rho'] = df[['bid_size', 'ask_size']].rolling(window=window_size).corr().unstack().iloc[:,1]\n\n    # 计算 best ask size 和 best bid size 之差\n    df['size_diff'] = df['ask_size'] - df['bid_size']\n    df['size_sum'] = df['ask_size'] + df['bid_size']\n\n    # 根据公式计算 p_up\n    df['p_up'] = np.arctan(np.sqrt((1 + df['rho']) / (1 - df['rho'])) * df['size_diff'] / df['size_sum']) / (2 * np.arctan(np.sqrt((1 + df['rho']) / (1 - df['rho']))))\n\n    # 计算 CL_up 和 CL_down\n    window_size_bolling = 11\n    window_size_cmo = 19\n\n    for price in prices:\n        # 计算布林带的滚动均值和标准差\n        rolling_mean = df[price].rolling(window=window_size_bolling).mean()\n        rolling_std = df[price].rolling(window=window_size_bolling).std()\n\n        # 布林带的上下限和中线\n        df[f'{price}_BB_upper'] = rolling_mean + 2 * rolling_std\n        df[f'{price}_BB_lower'] = rolling_mean - 2 * rolling_std\n        df[f'{price}_BB_middle'] = rolling_mean\n\n        # 计算价格的上涨和下跌信号\n        df[f'{price}_CL_up'] = (df[price] > df[price].shift(window_size_cmo)).astype(float)\n        df[f'{price}_CL_down'] = (df[price] < df[price].shift(window_size_cmo)).astype(float)\n\n        # 计算 S_u 和 S_d 的累加\n        df[f'{price}_S_u'] = (df[price] * df[f'{price}_CL_up']).rolling(window=window_size_cmo).sum()\n        df[f'{price}_S_d'] = (df[price] * df[f'{price}_CL_down']).rolling(window=window_size_cmo).sum()\n\n        # 计算 CMO 指标\n        df[f'{price}_CMO'] = 100 * (df[f'{price}_S_u'] - df[f'{price}_S_d']) / (df[f'{price}_S_u'] + df[f'{price}_S_d'])\n\n    return df\ntimeseries_features = timeseris_features_from_word(ORIGINAL_TRAIN.copy())","metadata":{"execution":{"iopub.status.busy":"2024-10-26T04:43:59.208793Z","iopub.execute_input":"2024-10-26T04:43:59.209400Z","iopub.status.idle":"2024-10-26T04:50:20.600644Z","shell.execute_reply.started":"2024-10-26T04:43:59.209341Z","shell.execute_reply":"2024-10-26T04:50:20.599077Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Function 'timeseris_features_from_word' executed in 380.7489 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"timeseries_features = pl.DataFrame(timeseries_features)","metadata":{"execution":{"iopub.status.busy":"2024-10-26T04:50:20.603158Z","iopub.execute_input":"2024-10-26T04:50:20.603577Z","iopub.status.idle":"2024-10-26T04:50:33.176155Z","shell.execute_reply.started":"2024-10-26T04:50:20.603522Z","shell.execute_reply":"2024-10-26T04:50:33.174867Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"timeseries_features.write_parquet(\"timeseries_features.parquet\")","metadata":{"execution":{"iopub.status.busy":"2024-10-26T04:50:33.177865Z","iopub.execute_input":"2024-10-26T04:50:33.178241Z","iopub.status.idle":"2024-10-26T04:51:28.021554Z","shell.execute_reply.started":"2024-10-26T04:50:33.178196Z","shell.execute_reply":"2024-10-26T04:51:28.020458Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# 3. 来自T9及图片中的因子——zwj","metadata":{}},{"cell_type":"code","source":"@time_it\ndef rank9_features_from_pic(df):\n    df_pl = pl.DataFrame(df)\n    # -------------pic--------------\n    base_features = [ 'imbalance_size','reference_price', 'matched_size',\n       'far_price', 'near_price', 'bid_price', 'bid_size', 'ask_price',\n       'ask_size', 'wap']\n    df_pl = df_pl.with_columns(pl.when(pl.col('seconds_in_bucket')<300).then(0).when(pl.col('seconds_in_bucket')<480).then(1).otherwise(2).cast(pl.Float32).alias(\"seconds_in_bucket_group\"))\n    df_pl = df_pl.with_columns([\n        (pl.col(col).first() / pl.col(col))\n        .over(['date_id', 'seconds_in_bucket_group', 'stock_id'])\n        .cast(pl.Float32)\n        .alias(f\"{col}_group_first_ratio\")\n        for col in base_features\n    ])\n    df_pl = df_pl.with_columns([\n        (pl.col(col).rolling_mean(window_size=100, min_periods=1) / pl.col(col))\n        .over(['date_id', 'seconds_in_bucket_group', 'stock_id'])\n        .cast(pl.Float32)\n        .alias(f\"{col}_group_expanding_mean_108\")\n        for col in base_features\n    ])\n    df_pl = df_pl.with_columns([\n        # 计算每列的分组平均值比率\n        (pl.col(col).mean() / pl.col(col))\n        .over(['date_id', 'seconds_in_bucket'])\n        .cast(pl.Float32)\n        .alias(f\"{col}_seconds_in_bucket_group_mean_ratio\")\n        for col in base_features\n    ] + [\n        # 计算每列的分组内排名比例\n        (pl.col(col).rank(descending=True, method='ordinal') / pl.col(col).count())\n        .over(['date_id', 'seconds_in_bucket'])\n        .cast(pl.Float32)\n        .alias(f\"{col}_seconds_in_bucket_group_rank\")\n        for col in base_features\n    ])\n    return df_pl\n    # -------------rank9--------------\n    \"\"\"\n    weights = [\n    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n    0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n    ]\n    weights = {int(k):v for k,v in enumerate(weights)}\n    df['weight']  = df['stock_id'].map(weights)\n    size_col = ['imbalance_size','matched_size','bid_size','ask_size']\n    for _ in size_col:\n        df[f'scale_{_}'] = df[_] / df.groupby(['stock_id'])[_].median()\n    # auc_ask_size、auc_ask_size，买卖单amount \n    df['auc_bid_size'] = df['matched_size']\n    df['auc_ask_size'] = df['matched_size']\n    df.loc[df['imbalance_buy_sell_flag']==1,'auc_bid_size'] += df.loc[df['imbalance_buy_sell_flag']==1,'imbalance_size']\n    df.loc[df['imbalance_buy_sell_flag']==-1,'auc_ask_size'] += df.loc[df['imbalance_buy_sell_flag']==-1,'imbalance_size']\n    df = pl.from_pandas(df)\n    feas_list = ['stock_id','seconds_in_bucket','imbalance_size','imbalance_buy_sell_flag',\n                'reference_price','matched_size','far_price','near_price','bid_price','bid_size',\n                'ask_price','ask_size','wap','scale_imbalance_size','scale_matched_size','scale_bid_size','scale_ask_size'\n                    ,'auc_bid_size','auc_ask_size']\n    # 基础特征\n    df = df.with_columns([\n        # 阶段1\n        (pl.col('ask_size') * pl.col('ask_price')).alias(\"ask_money\"),\n        (pl.col('bid_size') * pl.col('bid_price')).alias(\"bid_money\"),\n        (pl.col('ask_size') + pl.col(\"auc_ask_size\")).alias(\"ask_size_all\"),\n        (pl.col('bid_size') + pl.col(\"auc_bid_size\")).alias(\"bid_size_all\"),\n        (pl.col('ask_size') + pl.col(\"auc_ask_size\") + pl.col('bid_size') + pl.col(\"auc_bid_size\")).alias(\"volumn_size_all\"),\n        (pl.col('reference_price') * pl.col('auc_ask_size')).alias(\"ask_auc_money\"),\n        (pl.col('reference_price') * pl.col('auc_bid_size')).alias(\"bid_auc_money\"),\n        (pl.col('ask_size') * pl.col('ask_price') + pl.col('bid_size') * pl.col('bid_price')).alias(\"volumn_money\"),\n        (pl.col('ask_size') + pl.col('bid_size')).alias('volume_cont'),\n        (pl.col('ask_size') - pl.col('bid_size')).alias('diff_ask_bid_size'),\n        (pl.col('imbalance_size') + 2 * pl.col('matched_size')).alias('volumn_auc'),\n        ((pl.col('imbalance_size') + 2 * pl.col('matched_size')) * pl.col(\"reference_price\")).alias('volumn_auc_money'),\n        ((pl.col('ask_price') + pl.col('bid_price'))/2).alias('mid_price'),\n        ((pl.col('near_price') + pl.col('far_price'))/2).alias('mid_price_near_far'),\n        (pl.col('ask_price') - pl.col('bid_price')).alias('price_diff_ask_bid'),\n        (pl.col('ask_price') / pl.col('bid_price')).alias('price_div_ask_bid'),\n        (pl.col('imbalance_buy_sell_flag') * pl.col('scale_imbalance_size')).alias('flag_scale_imbalance_size'),\n        (pl.col('imbalance_buy_sell_flag') * pl.col('imbalance_size')).alias('flag_imbalance_size'),\n        (pl.col('imbalance_size') / pl.col('matched_size') * pl.col('imbalance_buy_sell_flag')).alias(\"div_flag_imbalance_size_2_balance\"),\n        ((pl.col('ask_price') - pl.col('bid_price')) * pl.col('imbalance_size')).alias('price_pressure'),\n        ((pl.col('ask_price') - pl.col('bid_price')) * pl.col('imbalance_size') * pl.col('imbalance_buy_sell_flag')).alias('price_pressure_v2'),\n        ((pl.col(\"ask_size\") - pl.col(\"bid_size\")) / (pl.col(\"far_price\") - pl.col(\"near_price\"))).alias(\"depth_pressure\"),\n        (pl.col(\"bid_size\") / pl.col(\"ask_size\")).alias(\"div_bid_size_ask_size\"),\n    ])\n    feas_list.extend(['ask_money', 'bid_money', 'ask_auc_money','bid_auc_money',\"ask_size_all\",\"bid_size_all\",\"volumn_size_all\",\n                        'volumn_money','volume_cont',\"volumn_auc\",\"volumn_auc_money\",\"mid_price\",\n                        'mid_price_near_far','price_diff_ask_bid',\"price_div_ask_bid\",\"flag_imbalance_size\",\"div_flag_imbalance_size_2_balance\",\n                        \"price_pressure\",\"price_pressure_v2\",\"depth_pressure\",\"flag_scale_imbalance_size\",\"diff_ask_bid_size\"]) \n     # 各种ratio\n    # 提升微忽几微\n    add_cols = []\n    for col1, col2 in [\n        (\"imbalance_size\",\"bid_size\"),\n        (\"imbalance_size\",\"ask_size\"),\n        (\"matched_size\",\"bid_size\"),\n        (\"matched_size\",\"ask_size\"),\n        (\"imbalance_size\",\"volume_cont\"),\n        (\"matched_size\",\"volume_cont\"),\n        (\"auc_bid_size\",\"bid_size\"),\n        (\"auc_ask_size\",\"ask_size\"),\n        (\"bid_auc_money\",\"bid_money\"),\n        (\"ask_auc_money\",\"ask_money\"),\n    ]:\n        add_cols.append((pl.col(col1) / pl.col(col2)).alias(f\"div_{col1}_2_{col2}\"))\n        feas_list.append(f\"div_{col1}_2_{col2}\")        \n    df = df.with_columns(add_cols)\n    # 阶段2 不平衡特征\n    # 除了price相关\n    # 没加auc的ask/bid的 构造price以及不平衡进去\n    add_cols = []\n    for pair1,pair2 in [\n        ('ask_size','bid_size'),\n        ('ask_money','bid_money'),\n        ('volumn_money','volumn_auc_money'),\n        ('volume_cont','volumn_auc'),\n        ('imbalance_size','matched_size'),\n        ('auc_ask_size','auc_bid_size'),\n        (\"ask_size_all\",'bid_size_all')\n    ]:\n        col_imb = f\"imb1_{pair1}_{pair2}\"\n        add_cols.extend([\n            ((pl.col(pair1) - pl.col(pair2)) / (pl.col(pair1) + pl.col(pair2))).alias(col_imb),\n        ])\n        feas_list.extend([col_imb])\n    df = df.with_columns(add_cols)\n\n\n    # price侧的imb1\n    fea_append_list = []\n    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\",\"mid_price\"]\n    for c in combinations(prices, 2):\n        fea_append_list.append(((pl.col(c[0]) - pl.col(c[1])) / (pl.col(c[0]) + pl.col(c[1]))).alias(f\"imb1_{c[0]}_{c[1]}\"))\n        # fea_append_list.append((pl.col(c[0]) - pl.col(c[1])).alias(f\"diff_{c[0]}_{c[1]}\"))\n        feas_list.extend([f\"imb1_{c[0]}_{c[1]}\"])\n    df = df.with_columns(fea_append_list)\n    del fea_append_list\n    # 不平衡特征 累计乘\n    df = df.with_columns([\n        ((pl.col(\"imb1_ask_size_bid_size\") + 2) * (pl.col(\"imb1_ask_price_bid_price\") + 2) * (pl.col(\"imb1_auc_ask_size_auc_bid_size\")+2)).alias(\"market_urgency_v2\"),\n        (pl.col('price_diff_ask_bid') * (pl.col('imb1_ask_size_bid_size'))).alias('market_urgency'),\n        (pl.col('imb1_ask_price_bid_price') * (pl.col('imb1_ask_size_bid_size'))).alias('market_urgency_v3'),\n    ])\n    feas_list.extend([f\"market_urgency_v3\",'market_urgency','market_urgency_v2'])\n\n    feas_list = ['imb1_wap_mid_price', 'imb1_ask_money_bid_money', 'imb1_volume_cont_volumn_auc', 'imb1_reference_price_ask_price', \n                    'imb1_reference_price_mid_price', 'seconds_in_bucket', 'div_flag_imbalance_size_2_balance', 'ask_price', \n                    'imb1_reference_price_bid_price', 'scale_matched_size', 'imb1_near_price_wap', 'volumn_auc_money', 'imb1_far_price_wap', \n                    'bid_size', 'scale_bid_size', 'bid_size_all']\n    # 隔离\n    add_cols = []\n    for col in [\"bid_auc_money\",\"imb1_reference_price_wap\",\"bid_size_all\",\n                \"imb1_auc_ask_size_auc_bid_size\",\"div_flag_imbalance_size_2_balance\",\n                \"imb1_ask_size_all_bid_size_all\",\"flag_imbalance_size\",\"imb1_reference_price_mid_price\"]:\n        for window in [3,6,18,36,60]:\n            add_cols.append(pl.col(col).rolling_mean(window_size=window,min_periods=1).over('stock_id','date_id').alias(f'rolling{window}_mean_{col}'))\n            add_cols.append(pl.col(col).rolling_std(window_size=window,min_periods=1).over('stock_id','date_id').alias(f'rolling{window}_std_{col}'))\n            feas_list.extend([f'rolling{window}_mean_{col}',f'rolling{window}_std_{col}'])\n    feas_list = ['imb1_wap_mid_price', 'imb1_ask_money_bid_money', 'imb1_volume_cont_volumn_auc', \n                        'imb1_reference_price_ask_price', 'imb1_reference_price_mid_price', \n                        'seconds_in_bucket', 'div_flag_imbalance_size_2_balance', 'ask_price', \n                        'imb1_reference_price_bid_price', 'scale_matched_size', 'imb1_near_price_wap', \n                        'volumn_auc_money', 'imb1_far_price_wap', 'bid_size', 'scale_bid_size', 'bid_size_all', \n                        'rolling18_mean_imb1_auc_ask_size_auc_bid_size', 'rolling3_mean_div_flag_imbalance_size_2_balance', \n                        'rolling60_std_div_flag_imbalance_size_2_balance', 'rolling36_mean_flag_imbalance_size', \n                        'rolling3_std_imb1_auc_ask_size_auc_bid_size', 'rolling18_mean_imb1_ask_size_all_bid_size_all', \n                        'rolling6_mean_div_flag_imbalance_size_2_balance', 'rolling6_std_imb1_auc_ask_size_auc_bid_size', \n                        'rolling3_mean_imb1_auc_ask_size_auc_bid_size', 'rolling60_std_imb1_auc_ask_size_auc_bid_size', \n                        'rolling6_std_bid_size_all', 'rolling3_std_bid_size_all', 'rolling3_mean_bid_size_all', \n                        'rolling18_std_bid_auc_money', 'rolling36_mean_bid_auc_money',\"rolling60_mean_imb1_reference_price_wap\",\n                    'rolling18_mean_imb1_reference_price_wap', 'rolling3_mean_imb1_reference_price_mid_price']\n    df = df.with_columns(add_cols)\n    add_cols = []\n    ### 杂七杂八\n    df = df.with_columns([\n        pl.col(\"flag_imbalance_size\").diff().over('stock_id','date_id').alias(\"imbalance_momentum_unscaled\"),\n        pl.col(\"price_diff_ask_bid\").diff().over('stock_id','date_id').alias(\"spread_intensity\"),\n    ])\n    feas_list.extend([\"imbalance_momentum_unscaled\",\"spread_intensity\"])\n    df = df.with_columns([\n        (pl.col(\"imbalance_momentum_unscaled\")/pl.col(\"matched_size\")).alias(\"imbalance_momentum\")\n    ])\n    feas_list.extend([\"imbalance_momentum\"])\n\n    #Calculate diff features for specific columns\n    add_cols = []\n    for col in ['ask_price',\n        'bid_price',\n        'imb1_reference_price_near_price',\n        'bid_size',\n        'scale_bid_size',\n        'mid_price',\n        'ask_size',\n        'price_div_ask_bid',\n        'div_bid_size_ask_size',\n        'market_urgency',\n        'wap',\n        'imbalance_momentum']:\n        for window in [1, 2, 3, 10]:\n            add_cols.append((pl.col(col).diff(window).over('stock_id','date_id')).alias(f\"{col}_diff_{window}\"))\n            feas_list.append(f\"{col}_diff_{window}\")\n    df = df.with_columns(add_cols)\n\n    ### target mock系列\n    for mock_period in [1,3,12,6]:\n\n        df = df.with_columns([\n            pl.col(\"wap\").shift(-mock_period).over(\"stock_id\",\"date_id\").alias(f\"wap_shift_n{mock_period}\")\n        ])\n        df = df.with_columns([\n            (pl.col(f\"wap_shift_n{mock_period}\")/pl.col(\"wap\")).alias(\"target_single\")\n        ])\n\n        tmp_df = df.select(pl.col(\"target_single\"),pl.col(\"weight\")).to_pandas()\n        tmp_df.loc[tmp_df[\"target_single\"].isna(),\"weight\"] = 0\n        df = df.with_columns([\n            pl.lit(np.array(tmp_df[\"weight\"])).alias(\"weight_tmp\")\n        ])\n\n        df = df.with_columns([\n            (((pl.col(\"weight_tmp\") * pl.col(\"target_single\")).sum().over(\"date_id\",\"seconds_in_bucket\")) / ((pl.col(\"weight_tmp\")).sum().over(\"date_id\",\"seconds_in_bucket\"))).alias(\"index_target_mock\")\n        ])\n\n        df = df.with_columns([\n            ((pl.col(\"target_single\") - pl.col(\"index_target_mock\"))*10000).alias(\"target_mock\")\n        ])\n\n        df = df.with_columns([\n            pl.col(\"target_mock\").shift(mock_period).over(\"stock_id\",\"date_id\").alias(f\"target_mock_shift{mock_period}\"),\n        ])\n\n    add_cols = []\n    for col in ['target_mock_shift6','target_mock_shift1','target_mock_shift3','target_mock_shift12']:\n        for window in [1, 3,6,12,24,48]:\n            add_cols.append(pl.col(col).rolling_mean(window_size=window,min_periods=1).over('stock_id','date_id').alias(f'rolling{window}_mean_{col}'))\n    df = df.with_columns(add_cols)\n    keep_cols_new = ['rolling48_mean_target_mock_shift3', 'rolling48_mean_target_mock_shift1', 'rolling48_mean_target_mock_shift12',\n    'rolling1_mean_target_mock_shift6', 'rolling24_mean_target_mock_shift6','rolling24_mean_target_mock_shift12',]\n    feas_list.extend(keep_cols_new)\n\n    add_cols = []\n    for col in [\"imb1_auc_ask_size_auc_bid_size\",\"flag_imbalance_size\",\"price_pressure_v2\",\"scale_matched_size\"]:\n        for window_size in [1,2,3,6,12]:\n            add_cols.append(pl.col(col).shift(window_size).over('stock_id','date_id').alias(f'shift{window_size}_{col}'))\n            add_cols.append((pl.col(col) / pl.col(col).shift(window_size).over('stock_id','date_id')).alias(f'div_shift{window_size}_{col}'))\n            add_cols.append((pl.col(col) - pl.col(col).shift(window_size).over('stock_id','date_id')).alias(f'diff_shift{window_size}_{col}'))\n    feas_list.extend(['div_shift6_imb1_auc_ask_size_auc_bid_size',\n    'diff_shift6_price_pressure_v2',\n    'shift1_price_pressure_v2',\n    'div_shift3_flag_imbalance_size',\n    'div_shift12_imb1_auc_ask_size_auc_bid_size',\n    'div_shift3_scale_matched_size',\n    'diff_shift6_flag_imbalance_size',\n    'shift12_imb1_auc_ask_size_auc_bid_size',\n    'div_shift12_price_pressure_v2',\n    'shift6_flag_imbalance_size',\n    'diff_shift3_imb1_auc_ask_size_auc_bid_size',\n    'div_shift12_flag_imbalance_size',\n    'shift12_flag_imbalance_size'])\n    df = df.with_columns(add_cols)\n\n    add_cols = []\n    for col in ['imb1_ask_price_mid_price',\n    'market_urgency',\n    'market_urgency_diff_1',\n    'imb1_ask_money_bid_money',\n    'rolling18_mean_imb1_ask_size_all_bid_size_all',\n    'rolling18_mean_imb1_auc_ask_size_auc_bid_size',\n    'rolling18_mean_imb1_reference_price_wap',\n    'ask_price_diff_3',\n    'diff_shift1_price_pressure_v2',\n    'diff_shift12_scale_matched_size',\n    'diff_shift1_flag_imbalance_size',\n    'imb1_ask_size_bid_size',\n    'imb1_bid_price_mid_price',\n    'rolling48_mean_target_mock_shift6']:\n        add_cols.append((((pl.col(col) * pl.col(\"weight\")).sum().over(\"date_id\",\"seconds_in_bucket\"))/(((pl.col(\"weight\")).sum().over(\"date_id\",\"seconds_in_bucket\")))).alias(f\"global_{col}\"))\n        feas_list.append(f\"global_{col}\")\n    df = df.with_columns(add_cols)\n\n\n    # MACD\n    rsi_cols = [\"mid_price_near_far\",\"imb1_reference_price_wap\",\"near_price\",]\n    add_cols = []\n    for col in rsi_cols:\n        for window_size in [3,6,12,24,48]:\n            add_cols.append(pl.col(col).ewm_mean(span=window_size, adjust=False).over('stock_id','date_id').alias(f\"rolling_ewm_{window_size}_{col}\"))\n    df = df.with_columns(add_cols)\n\n    add_cols = []\n    for col in rsi_cols:\n        for w1,w2 in zip((3,6,12,24),(6,12,24,48)):\n            add_cols.append((pl.col(f\"rolling_ewm_{w1}_{col}\") - pl.col(f\"rolling_ewm_{w2}_{col}\")).alias(f\"dif_{col}_{w1}_{w2}\"))\n    df = df.with_columns(add_cols)\n\n    add_cols = []\n    for col in rsi_cols:\n        for w1,w2 in zip((3,6,12,24),(6,12,24,48)):\n            add_cols.append(pl.col(f\"dif_{col}_{w1}_{w2}\").ewm_mean(span=9, adjust=False).over('stock_id','date_id').alias(f\"dea_{col}_{w1}_{w2}\"))\n    df = df.with_columns(add_cols)\n\n    add_cols = []\n    for col in rsi_cols:\n        for w1,w2 in zip((3,6,12,24),(6,12,24,48)):\n            add_cols.append((pl.col(f\"dif_{col}_{w1}_{w2}\") - pl.col(f\"dea_{col}_{w1}_{w2}\")).alias(f\"macd_{col}_{w1}_{w2}\"))\n\n    feas_list.extend(['macd_imb1_reference_price_wap_12_24',\n    'dif_imb1_reference_price_wap_3_6',\n    'macd_mid_price_near_far_12_24',\n    'dif_near_price_3_6',\n    'macd_near_price_24_48',\n    'dea_imb1_reference_price_wap_12_24',\n    'macd_near_price_12_24',\n    'rolling_ewm_24_imb1_reference_price_wap',\n    'dif_near_price_6_12',\n    'dea_mid_price_near_far_6_12',\n    'dea_near_price_24_48',\n    'rolling_ewm_12_imb1_reference_price_wap',\n    'dif_imb1_reference_price_wap_12_24'])\n    df = df.with_columns(add_cols)\n    # MACD\n    rsi_cols = [\"mid_price_near_far\",\"imb1_reference_price_wap\",\"near_price\",]\n    add_cols = []\n    for col in rsi_cols:\n        for window_size in [3,6,12,24,48]:\n            add_cols.append(pl.col(col).ewm_mean(span=window_size, adjust=False).over('stock_id','date_id').alias(f\"rolling_ewm_{window_size}_{col}\"))\n            #feas_list.append(f\"rolling_ewm_{window_size}_{col}\")\n    df = df.with_columns(add_cols)\n\n    add_cols = []\n    for col in rsi_cols:\n        for w1,w2 in zip((3,6,12,24),(6,12,24,48)):\n            add_cols.append((pl.col(f\"rolling_ewm_{w1}_{col}\") - pl.col(f\"rolling_ewm_{w2}_{col}\")).alias(f\"dif_{col}_{w1}_{w2}\"))\n            #feas_list.append(f\"dif_{col}_{w1}_{w2}\")\n    df = df.with_columns(add_cols)\n\n    add_cols = []\n    for col in rsi_cols:\n        for w1,w2 in zip((3,6,12,24),(6,12,24,48)):\n            add_cols.append(pl.col(f\"dif_{col}_{w1}_{w2}\").ewm_mean(span=9, adjust=False).over('stock_id','date_id').alias(f\"dea_{col}_{w1}_{w2}\"))\n            #feas_list.append(f\"dea_{col}_{w1}_{w2}\")\n    df = df.with_columns(add_cols)\n\n    add_cols = []\n    for col in rsi_cols:\n        for w1,w2 in zip((3,6,12,24),(6,12,24,48)):\n            add_cols.append((pl.col(f\"dif_{col}_{w1}_{w2}\") - pl.col(f\"dea_{col}_{w1}_{w2}\")).alias(f\"macd_{col}_{w1}_{w2}\"))\n            #feas_list.append(f\"macd_{col}_{w1}_{w2}\")\n\n    feas_list.extend(['macd_imb1_reference_price_wap_12_24',\n    'dif_imb1_reference_price_wap_3_6',\n    'macd_mid_price_near_far_12_24',\n    'dif_near_price_3_6',\n    'macd_near_price_24_48',\n    'dea_imb1_reference_price_wap_12_24',\n    'macd_near_price_12_24',\n    'rolling_ewm_24_imb1_reference_price_wap',\n    'dif_near_price_6_12',\n    'dea_mid_price_near_far_6_12',\n    'dea_near_price_24_48',\n    'rolling_ewm_12_imb1_reference_price_wap',\n    'dif_imb1_reference_price_wap_12_24'])\n    df = df.with_columns(add_cols)\n    del add_cols\n    tot_feas = df.join(df_pl,how='left',on=['stock_id','date_id','seconds_in_bucket'])\n    del df\n    return tot_feas\n    \"\"\"\nrank9_features = rank9_features_from_pic(ORIGINAL_TRAIN.copy())","metadata":{"execution":{"iopub.status.busy":"2024-10-26T05:19:37.179800Z","iopub.execute_input":"2024-10-26T05:19:37.180277Z","iopub.status.idle":"2024-10-26T05:20:04.715631Z","shell.execute_reply.started":"2024-10-26T05:19:37.180223Z","shell.execute_reply":"2024-10-26T05:20:04.714135Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Function 'rank9_features_from_pic' executed in 27.1509 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"rank9_features.write_parquet(\"rank9_features.parquet\")","metadata":{"execution":{"iopub.status.busy":"2024-10-26T05:03:32.098323Z","iopub.execute_input":"2024-10-26T05:03:32.100596Z","iopub.status.idle":"2024-10-26T05:06:00.034208Z","shell.execute_reply.started":"2024-10-26T05:03:32.100528Z","shell.execute_reply":"2024-10-26T05:06:00.032365Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# 4.ipynb中的因子——wyn","metadata":{}},{"cell_type":"code","source":"@njit(parallel=True)\ndef compute_triplet_imbalance(df_values, comb_indices):\n    num_rows = df_values.shape[0]\n    num_combinations = len(comb_indices)\n    imbalance_features = np.empty((num_rows, num_combinations))\n\n    for i in prange(num_combinations):\n        a, b, c = comb_indices[i]\n        for j in range(num_rows):\n            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n            if mid_val == min_val:  # Prevent division by zero\n                imbalance_features[j, i] = np.nan\n            else:\n                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n\n    return imbalance_features\ndef calculate_triplet_imbalance_numba(price, df):\n    # Convert DataFrame to numpy array for Numba compatibility\n    df_values = df[price].values\n    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n\n    # Calculate the triplet imbalance\n    features_array = compute_triplet_imbalance(df_values, comb_indices)\n\n    # Create a DataFrame from the results\n    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n    features = pd.DataFrame(features_array, columns=columns)\n\n    return features","metadata":{"execution":{"iopub.status.busy":"2024-10-26T05:27:35.473123Z","iopub.execute_input":"2024-10-26T05:27:35.473601Z","iopub.status.idle":"2024-10-26T05:27:35.579062Z","shell.execute_reply.started":"2024-10-26T05:27:35.473545Z","shell.execute_reply":"2024-10-26T05:27:35.577848Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"@time_it\ndef features_from_ipynb(df):\n    def generate_features(df):\n    \n        prices = ['reference_price', 'far_price', 'mid_price',\n                  'near_price', 'ask_price', 'bid_price', 'wap']\n        sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n\n        df['mid_price'] = (df['ask_price']+df['bid_price'])/2\n\n        for i, a in enumerate(prices):\n            for j, b in enumerate(prices):\n                if i > j:\n                    df[f'{a}_{b}_imb'] = (df[a]-df[b])/(df[a]+df[b])\n\n        for c in [prices, sizes]:\n            triplet_feature = calculate_triplet_imbalance_numba(c, df)\n            df[triplet_feature.columns] = triplet_feature.values\n\n        df['spread'] = df['ask_price']-df['bid_price']\n        df['price_pressure'] = df['imbalance_size']*df['spread']\n        df['ask_amount'] = df['ask_price']*df['ask_size']\n        df['bid_amount'] = df['bid_price']*df['bid_size']\n        df['marketdepth'] = df['ask_size']+df['bid_size']\n        df['liquidity_imbalance'] = (\n            df['bid_size']-df['ask_size'])/df['marketdepth']\n        df['matched_imbalance'] = (df['imbalance_size']-df['matched_size']) / \\\n            (df['imbalance_size']+df['matched_size'])\n        df['wap_2'] = (df['ask_amount']+df['bid_amount'])/df['marketdepth']\n        df['bboimb'] = (df['bid_amount']-df['ask_amount']) / \\\n            (df['ask_amount']+df['bid_amount'])\n        df['price_imbalance'] = df['bid_price']/df['ask_price']\n        df['size_imbalance'] = df['bid_size']/df['ask_size']\n        df['market_urgency'] = df['spread'] * df['liquidity_imbalance']\n        df['effectiveSpread'] = np.abs(df['wap']-df['mid_price'])/df['mid_price']\n        df['shallowLIX'] = np.log(\n            df['matched_size']/(df['ask_price']-df['bid_price']))\n        df['imbalance_ratio'] = df['imbalance_size'] / \\\n            (df['imbalance_size']+df['matched_size'])\n        df['logquoteslope'] = (np.log(df['ask_price'])-np.log(df['bid_price'])) / \\\n            (np.log(df['ask_size'])+np.log(df['bid_size']))\n        df['wapm'] = (df['wap_2']-df['mid_price'])/df['mid_price']\n        df['MCI'] = df['wapm']/(df['ask_amount']+df['bid_amount'])\n        df['imb_size_with_flag'] = (2 * df['imbalance_size'] *\n                                    df['imbalance_buy_sell_flag'])/df['marketdepth']\n        df['farrefliquidity'] = (\n            df['far_price']/df['reference_price']-1)/df['matched_size']\n        df['farmidliquidity'] = (\n            df['far_price']/df['mid_price']-1)/df['matched_size']\n        df['farnearliquidity'] = (\n            df['far_price']/df['near_price']-1)/df['matched_size']\n        df['farwapliquidity'] = (df['far_price']/df['wap']-1)/df['matched_size']\n        df['depth_pressure'] = (df['ask_size']-df['bid_size']) * \\\n            (df['far_price']-df['near_price'])\n        return df\n    \n    def cal_extra_fea(df_original_fea,\n                  window=[6, 12],\n                  diff_window=[1, 2, 3, 5, 10],\n                  fac_to_remain=None):\n        '''\n        df_original_fea: pd.Series, index为datetime和security, name为因子名称\n        window: list\n        fac_to_remain: list, 指保留的因子名称\n        '''\n        f = df_original_fea.name\n        fea_wide = df_original_fea.unstack().T\n\n        feature = pd.DataFrame(index=df_original_fea.index)\n\n        # 预先计算 rolling 对象以减少重复计算\n        rolling_windows = {w: fea_wide.rolling(w, min_periods=1) for w in window}\n\n        for w in diff_window:\n            # 计算差分 (diff)\n            fname = f'{f}_diff{w}'\n            if fac_to_remain is None or fname in fac_to_remain:\n                feature[fname] = (fea_wide - fea_wide.shift(w)).stack(dropna=False)\n\n            # 计算动量 (momentum)\n            fname = f'{f}_mom{w}'\n            if fac_to_remain is None or fname in fac_to_remain:\n                feature[fname] = (fea_wide / fea_wide.shift(w) - 1).stack(dropna=False)\n\n        for w, roll in rolling_windows.items():\n            # 滚动均值 (ma)\n            fname = f'{f}_ma{w}'\n            if fac_to_remain is None or fname in fac_to_remain:\n                feature[fname] = roll.mean().stack(dropna=False)\n\n            # 80%分位数 (qtlu)\n            fname = f'{f}_qtlu{w}'\n            if fac_to_remain is None or fname in fac_to_remain:\n                feature[fname] = roll.quantile(0.8).stack(dropna=False)\n\n            # 20%分位数 (qtld)\n            fname = f'{f}_qtld{w}'\n            if fac_to_remain is None or fname in fac_to_remain:\n                feature[fname] = roll.quantile(0.2).stack(dropna=False)\n\n            # 排名百分比 (rank)\n            fname = f'{f}_rank{w}'\n            if fac_to_remain is None or fname in fac_to_remain:\n                feature[fname] = roll.rank(pct=True).stack(dropna=False)\n\n            # 标准差 (std)\n            fname = f'{f}_std{w}'\n            if fac_to_remain is None or fname in fac_to_remain:\n                feature[fname] = roll.std().stack(dropna=False)\n\n            # 最大值 (max)\n            fname = f'{f}_max{w}'\n            if fac_to_remain is None or fname in fac_to_remain:\n                feature[fname] = roll.max().stack(dropna=False)\n\n            # 最小值 (low)\n            fname = f'{f}_low{w}'\n            if fac_to_remain is None or fname in fac_to_remain:\n                feature[fname] = roll.min().stack(dropna=False)\n        return feature\n\n#             # 计算最大值的相对位置 (imax)\n#             fname = f'{f}_imax{w}'\n\n#             def findMaxIdx(series): \n#                 return series.shape[0] - series.reset_index(drop=True).idxmax()\n\n#             if fac_to_remain is None or fname in fac_to_remain:\n#                 feature[fname] = roll.apply(findMaxIdx).stack(dropna=False) / w\n\n#             # 计算最小值的相对位置 (imin) \n#             fname = f'{f}_imin{w}'\n\n#             def findMinIdx(series): \n#                 return series.shape[0] - series.reset_index(drop=True).idxmin()\n\n#             if fac_to_remain is None or fname in fac_to_remain:\n#                 feature[fname] = roll.apply(findMinIdx).stack(dropna=False) / w\n            \n    df1 = generate_features(df.copy())\n    result = df1\n    print('---------df1 done-----------')\n    \"\"\"\n    df2 = df.copy()\n    df2.set_index(['stock_id', 'time_id'], inplace=True)\n    for f in ['imbalance_size','reference_price','matched_size','near_price','bid_price','bid_size','ask_price','ask_size','wap']:\n        extra_fea = cal_extra_fea(df2[f],window=[6, 12],diff_window=[1, 5, 10],fac_to_remain=None)\n        extra_fea = extra_fea.reset_index()\n        print(result.shape,extra_fea.shape)\n        result = pd.merge(result,extra_fea,how='left',on=['stock_id','time_id'])\n    \"\"\"\n    return result\nipynb_features = features_from_ipynb(ORIGINAL_TRAIN.copy())","metadata":{"execution":{"iopub.status.busy":"2024-10-26T05:27:35.828881Z","iopub.execute_input":"2024-10-26T05:27:35.829297Z","iopub.status.idle":"2024-10-26T05:27:48.774365Z","shell.execute_reply.started":"2024-10-26T05:27:35.829256Z","shell.execute_reply":"2024-10-26T05:27:48.772658Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"---------df1 done-----------\nFunction 'features_from_ipynb' executed in 12.4721 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"ipynb_features = pl.DataFrame(ipynb_features)","metadata":{"execution":{"iopub.status.busy":"2024-10-26T05:28:42.140063Z","iopub.execute_input":"2024-10-26T05:28:42.140541Z","iopub.status.idle":"2024-10-26T05:28:49.205403Z","shell.execute_reply.started":"2024-10-26T05:28:42.140497Z","shell.execute_reply":"2024-10-26T05:28:49.204133Z"},"trusted":true},"execution_count":8,"outputs":[]}]}