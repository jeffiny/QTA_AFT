{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cef0d23",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-12-20T07:25:23.772050Z",
     "iopub.status.busy": "2023-12-20T07:25:23.771097Z",
     "iopub.status.idle": "2023-12-20T07:25:30.463461Z",
     "shell.execute_reply": "2023-12-20T07:25:30.461378Z"
    },
    "papermill": {
     "duration": 6.705135,
     "end_time": "2023-12-20T07:25:30.467409",
     "exception": false,
     "start_time": "2023-12-20T07:25:23.762274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import copy\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import weight_norm\n",
    "from numba import njit, prange\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import optiver2023\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "model_path ='/kaggle/input/optiver-models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f70394fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-20T07:25:30.484638Z",
     "iopub.status.busy": "2023-12-20T07:25:30.483664Z",
     "iopub.status.idle": "2023-12-20T07:25:30.652201Z",
     "shell.execute_reply": "2023-12-20T07:25:30.650695Z"
    },
    "papermill": {
     "duration": 0.181257,
     "end_time": "2023-12-20T07:25:30.655602",
     "exception": false,
     "start_time": "2023-12-20T07:25:30.474345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_features(df):\n",
    "    prices = ['reference_price', 'far_price', 'mid_price',\n",
    "              'near_price', 'ask_price', 'bid_price', 'wap']\n",
    "    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n",
    "    \n",
    "    df['mid_price'] = (df['ask_price']+df['bid_price'])/2\n",
    "    \n",
    "    for i, a in enumerate(prices):\n",
    "        for j, b in enumerate(prices):\n",
    "            if i > j:\n",
    "                df[f'{a}_{b}_imb'] = (df[a]-df[b])/(df[a]+df[b])\n",
    "\n",
    "    for c in [prices, sizes]:\n",
    "        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n",
    "        df[triplet_feature.columns] = triplet_feature.values\n",
    "\n",
    "    df['spread'] = df['ask_price']-df['bid_price']\n",
    "    df['price_pressure'] = df['imbalance_size']*df['spread']\n",
    "    df['ask_amount'] = df['ask_price']*df['ask_size']\n",
    "    df['bid_amount'] = df['bid_price']*df['bid_size']\n",
    "    df['marketdepth'] = df['ask_size']+df['bid_size']\n",
    "    df['liquidity_imbalance'] = (\n",
    "        df['bid_size']-df['ask_size'])/df['marketdepth']\n",
    "    df['matched_imbalance'] = (df['imbalance_size']-df['matched_size']) / \\\n",
    "        (df['imbalance_size']+df['matched_size'])\n",
    "    df['wap_2'] = (df['ask_amount']+df['bid_amount'])/df['marketdepth']\n",
    "    df['bboimb'] = (df['bid_amount']-df['ask_amount']) / \\\n",
    "        (df['ask_amount']+df['bid_amount'])\n",
    "    df['price_imbalance'] = df['bid_price']/df['ask_price']\n",
    "    df['size_imbalance'] = df['bid_size']/df['ask_size']\n",
    "    df['market_urgency'] = df['spread'] * df['liquidity_imbalance']\n",
    "    df['effectiveSpread'] = np.abs(df['wap']-df['mid_price'])/df['mid_price']\n",
    "    df['shallowLIX'] = np.log(\n",
    "        df['matched_size']/(df['ask_price']-df['bid_price']))\n",
    "    df['imbalance_ratio'] = df['imbalance_size'] / \\\n",
    "        (df['imbalance_size']+df['matched_size'])\n",
    "    df['logquoteslope'] = (np.log(df['ask_price'])-np.log(df['bid_price'])) / \\\n",
    "        (np.log(df['ask_size'])+np.log(df['bid_size']))\n",
    "    df['wapm'] = (df['wap_2']-df['mid_price'])/df['mid_price']\n",
    "    df['MCI'] = df['wapm']/(df['ask_amount']+df['bid_amount'])\n",
    "    df['imb_size_with_flag'] = (2 * df['imbalance_size'] *\n",
    "                                df['imbalance_buy_sell_flag'])/df['marketdepth']\n",
    "    df['farrefliquidity'] = (\n",
    "        df['far_price']/df['reference_price']-1)/df['matched_size']\n",
    "    df['farmidliquidity'] = (\n",
    "        df['far_price']/df['mid_price']-1)/df['matched_size']\n",
    "    df['farnearliquidity'] = (\n",
    "        df['far_price']/df['near_price']-1)/df['matched_size']\n",
    "    df['farwapliquidity'] = (df['far_price']/df['wap']-1)/df['matched_size']\n",
    "    df['depth_pressure'] = (df['ask_size']-df['bid_size']) * \\\n",
    "        (df['far_price']-df['near_price'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def market_features(df_1day):\n",
    "    weights = [\n",
    "        0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n",
    "        0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n",
    "        0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n",
    "        0.004, 0.004, 0.006, 0.002, 0.002, 0.04, 0.002, 0.002, 0.004, 0.04, 0.002, 0.001,\n",
    "        0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n",
    "        0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n",
    "        0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n",
    "        0.02, 0.004, 0.006, 0.002, 0.02, 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n",
    "        0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n",
    "        0.004, 0.006, 0.006, 0.001, 0.04, 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n",
    "        0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n",
    "        0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n",
    "        0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n",
    "        0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n",
    "        0.04, 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02, 0.004, 0.002, 0.006, 0.02,\n",
    "        0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n",
    "        0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n",
    "    ]\n",
    "    weights = pd.Series(weights)\n",
    "    weights.index.name = 'stock_id'\n",
    "\n",
    "    market_feats = pd.DataFrame(\n",
    "        index=df_1day.index.get_level_values(0).unique())\n",
    "    wap = df_1day['wap'].unstack()\n",
    "    market_feats['index_wap_std'] = wap.std(axis=1)\n",
    "    stock_ret1 = 10000*(wap/wap.shift(1)-1)\n",
    "    market_feats['index_ret1'] = stock_ret1.mean(axis=1)\n",
    "    market_feats['index_ret1_std'] = stock_ret1.std(axis=1)\n",
    "    market_feats['index_ret1_w'] = stock_ret1.mul(weights, axis=1).sum(axis=1)\n",
    "    stock_ret6 = 10000*(wap/wap.shift(6)-1)\n",
    "    market_feats['index_ret6'] = stock_ret6.mean(axis=1)\n",
    "    market_feats['index_ret6_std'] = stock_ret6.std(axis=1)\n",
    "    market_feats['index_ret6_w'] = stock_ret6.mul(weights, axis=1).sum(axis=1)\n",
    "    ask_size = df_1day['ask_size'].unstack()\n",
    "    bid_size = df_1day['bid_size'].unstack()\n",
    "    market_feats['market_volume'] = ask_size.mean(axis=1)+bid_size.mean(axis=1)\n",
    "    ask_price = df_1day['ask_price'].unstack()\n",
    "    bid_price = df_1day['bid_price'].unstack()\n",
    "    market_feats['market_spread'] = ask_price.mean(\n",
    "        axis=1)-bid_price.mean(axis=1)\n",
    "    market_feats['market_max_spread'] = ask_price.max(\n",
    "        axis=1)-bid_price.min(axis=1)\n",
    "    market_feats['mfd'] = market_feats.index // 60\n",
    "    return market_feats\n",
    "\n",
    "\n",
    "def cal_extra_fea(df_original_fea,\n",
    "                  window=[6, 12],\n",
    "                  diff_window=[1, 2, 3, 5, 10],\n",
    "                  fac_to_remain=None,\n",
    "                  SUB_MODE=False):\n",
    "    '''\n",
    "    df_original_fea: pd.Series, index为datetime和security, name为因子名称\n",
    "    window: list\n",
    "    fac_to_remain: list, 指保留的因子名称\n",
    "     '''\n",
    "    f = df_original_fea.name\n",
    "    fea_wide = df_original_fea.unstack()\n",
    "    # 用于生成本地因子\n",
    "    if not SUB_MODE:\n",
    "        feature = pd.DataFrame(index=df_original_fea.index)\n",
    "        for w in diff_window:\n",
    "            # diff\n",
    "            fname = f'{f}_diff{w}'\n",
    "            if fac_to_remain is None or fname in fac_to_remain:\n",
    "                fea_tmp = fea_wide-fea_wide.shift(w)\n",
    "                feature[fname] = fea_tmp.stack(dropna=False)\n",
    "            # mom\n",
    "            fname = f'{f}_mom{w}'\n",
    "            if fac_to_remain is None or fname in fac_to_remain:\n",
    "                fea_tmp = fea_wide/fea_wide.shift(w)-1\n",
    "                feature[fname] = fea_tmp.stack(dropna=False)\n",
    "        for w in window:\n",
    "            # ma\n",
    "            fname = f'{f}_ma{w}'\n",
    "            if fac_to_remain is None or fname in fac_to_remain:\n",
    "                fea_tmp = fea_wide.rolling(w, min_periods=1).mean()\n",
    "                feature[fname] = fea_tmp.stack(dropna=False)\n",
    "            # qtlu\n",
    "            fname = f'{f}_qtlu{w}'\n",
    "            if fac_to_remain is None or fname in fac_to_remain:\n",
    "                fea_tmp = fea_wide.rolling(w, min_periods=1).quantile(0.8)\n",
    "                feature[fname] = fea_tmp.stack(dropna=False)\n",
    "            # qtld\n",
    "            fname = f'{f}_qtld{w}'\n",
    "            if fac_to_remain is None or fname in fac_to_remain:\n",
    "                fea_tmp = fea_wide.rolling(w, min_periods=1).quantile(0.2)\n",
    "                feature[fname] = fea_tmp.stack(dropna=False)\n",
    "            # rank\n",
    "            fname = f'{f}_rank{w}'\n",
    "            if fac_to_remain is None or fname in fac_to_remain:\n",
    "                fea_tmp = fea_wide.rolling(w, min_periods=1).rank(pct=True)\n",
    "                feature[fname] = fea_tmp.stack(dropna=False)\n",
    "            # std\n",
    "            fname = f'{f}_std{w}'\n",
    "            if fac_to_remain is None or fname in fac_to_remain:\n",
    "                fea_tmp = fea_wide.rolling(w, min_periods=1).std()\n",
    "                feature[fname] = fea_tmp.stack(dropna=False)\n",
    "            # max\n",
    "            fname = f'{f}_max{w}'\n",
    "            if fac_to_remain is None or fname in fac_to_remain:\n",
    "                fea_tmp = fea_wide.rolling(w, min_periods=1).max()\n",
    "                feature[fname] = fea_tmp.stack(dropna=False)\n",
    "            # low\n",
    "            fname = f'{f}_low{w}'\n",
    "            if fac_to_remain is None or fname in fac_to_remain:\n",
    "                fea_tmp = fea_wide.rolling(w, min_periods=1).min()\n",
    "                feature[fname] = fea_tmp.stack(dropna=False)\n",
    "            # IMAX\n",
    "            fname = f'{f}_imax{w}'\n",
    "\n",
    "            def findMaxIdx(\n",
    "                series): return series.shape[0] - series.reset_index(drop=True).idxmax()\n",
    "            if fac_to_remain is None or fname in fac_to_remain:\n",
    "                fea_tmp = fea_wide.rolling(\n",
    "                    w, min_periods=1).apply(findMaxIdx) / w\n",
    "                feature[fname] = fea_tmp.stack(dropna=False)\n",
    "            # IMIN\n",
    "            fname = f'{f}_imin{w}'\n",
    "\n",
    "            def findMinIdx(\n",
    "                series): return series.shape[0] - series.reset_index(drop=True).idxmin()\n",
    "            if fac_to_remain is None or fname in fac_to_remain:\n",
    "                fea_tmp = fea_wide.rolling(\n",
    "                    w, min_periods=1).apply(findMinIdx) / w\n",
    "                feature[fname] = fea_tmp.stack(dropna=False)\n",
    "    # 用于推理\n",
    "    else:\n",
    "        feature = pd.DataFrame(index=fea_wide.columns.to_list())\n",
    "        feature.index.name = 'stock_id'\n",
    "        for w in diff_window:\n",
    "            # diff\n",
    "            fname = f'{f}_diff{w}'\n",
    "            if fac_to_remain is None or fname in fac_to_remain:\n",
    "                try:\n",
    "                    fea_tmp = fea_wide.iloc[-1] - fea_wide.iloc[-(w+1)]\n",
    "                    feature[fname] = fea_tmp\n",
    "                except:\n",
    "                    feature[fname] = np.nan\n",
    "            # mom\n",
    "            fname = f'{f}_mom{w}'\n",
    "            if fac_to_remain is None or fname in fac_to_remain:\n",
    "                try:\n",
    "                    fea_tmp = fea_wide.iloc[-1] / fea_wide.iloc[-(w+1)] - 1\n",
    "                    feature[fname] = fea_tmp\n",
    "                except:\n",
    "                    feature[fname] = np.nan\n",
    "        for w in window:\n",
    "            # ma\n",
    "            fname = f'{f}_ma{w}'\n",
    "            if fac_to_remain is None or fname in fac_to_remain:\n",
    "                try:\n",
    "                    # fea_tmp = fea_wide.iloc[-w:].mean()\n",
    "                    fea_tmp = fea_wide.iloc[-w:].rolling(\n",
    "                        w, min_periods=1).mean().iloc[-1, :]\n",
    "                    feature[fname] = fea_tmp\n",
    "                except:\n",
    "                    feature[fname] = np.nan\n",
    "            # qtlu\n",
    "            fname = f'{f}_qtlu{w}'\n",
    "            if fac_to_remain is None or fname in fac_to_remain:\n",
    "                try:\n",
    "                    fea_tmp = fea_wide.iloc[-w:].quantile(0.8)\n",
    "                    feature[fname] = fea_tmp\n",
    "                except:\n",
    "                    feature[fname] = np.nan\n",
    "            # qtld\n",
    "            fname = f'{f}_qtld{w}'\n",
    "            if fac_to_remain is None or fname in fac_to_remain:\n",
    "                try:\n",
    "                    fea_tmp = fea_wide.iloc[-w:].quantile(0.2)\n",
    "                    feature[fname] = fea_tmp\n",
    "                except:\n",
    "                    feature[fname] = np.nan\n",
    "            # rank\n",
    "            fname = f'{f}_rank{w}'\n",
    "            if fac_to_remain is None or fname in fac_to_remain:\n",
    "                try:\n",
    "                    fea_tmp = fea_wide.iloc[-w:].rank(pct=True).iloc[-1]\n",
    "                    feature[fname] = fea_tmp\n",
    "                except:\n",
    "                    feature[fname] = np.nan\n",
    "            # std\n",
    "            fname = f'{f}_std{w}'\n",
    "            if fac_to_remain is None or fname in fac_to_remain:\n",
    "                try:\n",
    "                    # fea_tmp = fea_wide.iloc[-w:].std()\n",
    "                    fea_tmp = fea_wide.iloc[-w:].rolling(\n",
    "                        w, min_periods=1).std().iloc[-1, :]\n",
    "                    feature[fname] = fea_tmp\n",
    "                except:\n",
    "                    feature[fname] = np.nan\n",
    "            # max\n",
    "            fname = f'{f}_max{w}'\n",
    "            if fac_to_remain is None or fname in fac_to_remain:\n",
    "                try:\n",
    "                    fea_tmp = fea_wide.iloc[-w:].max()\n",
    "                    feature[fname] = fea_tmp\n",
    "                except:\n",
    "                    feature[fname] = np.nan\n",
    "            # low\n",
    "            fname = f'{f}_low{w}'\n",
    "            if fac_to_remain is None or fname in fac_to_remain:\n",
    "                try:\n",
    "                    fea_tmp = fea_wide.iloc[-w:].min()\n",
    "                    feature[fname] = fea_tmp\n",
    "                except:\n",
    "                    feature[fname] = np.nan\n",
    "            # IMAX\n",
    "            fname = f'{f}_imax{w}'\n",
    "\n",
    "            def findMaxIdx(\n",
    "                series): return series.shape[0] - series.reset_index(drop=True).idxmax()\n",
    "            if fac_to_remain is None or fname in fac_to_remain:\n",
    "                try:\n",
    "                    fea_tmp = fea_wide.iloc[-w:].apply(findMaxIdx) / w\n",
    "                    feature[fname] = fea_tmp\n",
    "                except:\n",
    "                    feature[fname] = np.nan\n",
    "            # IMIN\n",
    "            fname = f'{f}_imin{w}'\n",
    "\n",
    "            def findMinIdx(\n",
    "                series): return series.shape[0] - series.reset_index(drop=True).idxmin()\n",
    "            if fac_to_remain is None or fname in fac_to_remain:\n",
    "                try:\n",
    "                    fea_tmp = fea_wide.iloc[-w:].apply(findMinIdx) / w\n",
    "                    feature[fname] = fea_tmp\n",
    "                except:\n",
    "                    feature[fname] = np.nan\n",
    "    return feature\n",
    "\n",
    "\n",
    "@njit(parallel=True)\n",
    "def compute_triplet_imbalance(df_values, comb_indices):\n",
    "    num_rows = df_values.shape[0]\n",
    "    num_combinations = len(comb_indices)\n",
    "    imbalance_features = np.empty((num_rows, num_combinations))\n",
    "\n",
    "    for i in prange(num_combinations):\n",
    "        a, b, c = comb_indices[i]\n",
    "        for j in range(num_rows):\n",
    "            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n",
    "            if mid_val == min_val:  # Prevent division by zero\n",
    "                imbalance_features[j, i] = np.nan\n",
    "            else:\n",
    "                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n",
    "\n",
    "    return imbalance_features\n",
    "\n",
    "\n",
    "def calculate_triplet_imbalance_numba(price, df):\n",
    "    # Convert DataFrame to numpy array for Numba compatibility\n",
    "    df_values = df[price].values\n",
    "    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n",
    "\n",
    "    # Calculate the triplet imbalance\n",
    "    features_array = compute_triplet_imbalance(df_values, comb_indices)\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n",
    "    features = pd.DataFrame(features_array, columns=columns)\n",
    "\n",
    "    return features\n",
    "\n",
    "def huber_approx_obj(dtrain, preds):\n",
    "    d = preds - dtrain  \n",
    "    h = 1 #h is delta in the formula  \n",
    "    scale = 1 + (d / h) ** 2  \n",
    "    scale_sqrt = np.sqrt(scale)  \n",
    "    grad = d / scale_sqrt  \n",
    "    hess = 1 / scale / scale_sqrt  \n",
    "    return grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0a0b7b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-20T07:25:31.036698Z",
     "iopub.status.busy": "2023-12-20T07:25:31.036155Z",
     "iopub.status.idle": "2023-12-20T07:25:31.095043Z",
     "shell.execute_reply": "2023-12-20T07:25:31.093196Z"
    },
    "papermill": {
     "duration": 0.071337,
     "end_time": "2023-12-20T07:25:31.099320",
     "exception": false,
     "start_time": "2023-12-20T07:25:31.027983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203 ['effectiveSpread', 'MCI', 'wap_reference_price_imb'] ['reference_price_std12', 'ask_price_diff2', 'bid_price_reference_price_imb_diff2']\n",
      "89 ['wap_near_price_imb', 'bid_price_near_price_imb', 'wap_near_price_imb_diff1'] ['wap_ask_price_imb_max12', 'near_price_reference_price_imb_diff10', 'farmidliquidity_diff2']\n",
      "292 ['matched_size_bid_size_imbalance_size_imb2_diff10', 'reference_price_ask_price_bid_price_imb2_std12', 'mfd'] ['wap_ask_price_imb_qtlu12', 'spread', 'wap_reference_price_imb_low6']\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "feature_lst0 = pd.read_csv(Path(model_path, 'feature_importance0_final.csv'), \n",
    "                           index_col=0).index.to_list()\n",
    "feature_lst0_RSR = deepcopy(feature_lst0)\n",
    "if 'mfd' in feature_lst0_RSR:\n",
    "    feature_lst0_RSR.remove('mfd')\n",
    "    feature_lst0_RSR = ['mfd',] + feature_lst0_RSR    # 把需要embedding的放在最前面\n",
    "print(len(feature_lst0), feature_lst0[:3], feature_lst0[-3:])\n",
    "\n",
    "\n",
    "feature_lst1 = pd.read_csv(Path(model_path, 'feature_importance1_final.csv'), \n",
    "                           index_col=0).index.to_list()\n",
    "###################################\n",
    "original_RSR_feature_lst1 = deepcopy(feature_lst1)\n",
    "if 'mfd' in original_RSR_feature_lst1:\n",
    "    original_RSR_feature_lst1.remove('mfd')\n",
    "    original_RSR_feature_lst1 = ['mfd',] + original_RSR_feature_lst1    # 把需要embedding的放在最前面\n",
    "###################################\n",
    "feature_lst1 = [x for x in feature_lst1 if x not in feature_lst0]\n",
    "feature_lst1_RSR = deepcopy(feature_lst1)\n",
    "if 'mfd' in feature_lst1_RSR:\n",
    "    feature_lst1_RSR.remove('mfd')\n",
    "    feature_lst1_RSR = ['mfd',] + feature_lst1_RSR    # 把需要embedding的放在最前面\n",
    "print(len(feature_lst1), feature_lst1[:3], feature_lst1[-3:])\n",
    "\n",
    "\n",
    "feature_lst01 = list(set(feature_lst0+feature_lst1))\n",
    "print(len(feature_lst01), feature_lst01[:3], feature_lst01[-3:])\n",
    "\n",
    "\n",
    "norm_col_lst0 = ['index_wap_std', 'index_ret1', 'index_ret1_std', 'index_ret1_w', 'index_ret6', \n",
    "                 'index_ret6_std', 'index_ret6_w', 'market_volume', 'market_spread', 'market_max_spread']\n",
    "norm_col_lst1 = [x for x in norm_col_lst0 if x not in feature_lst0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "374e0fed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-20T07:26:42.815686Z",
     "iopub.status.busy": "2023-12-20T07:26:42.814455Z",
     "iopub.status.idle": "2023-12-20T07:26:42.820468Z",
     "shell.execute_reply": "2023-12-20T07:26:42.819457Z"
    },
    "papermill": {
     "duration": 0.024538,
     "end_time": "2023-12-20T07:26:42.823247",
     "exception": false,
     "start_time": "2023-12-20T07:26:42.798709",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = optiver2023.make_env()\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd53641a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-20T07:26:42.850978Z",
     "iopub.status.busy": "2023-12-20T07:26:42.850485Z",
     "iopub.status.idle": "2023-12-20T07:26:43.927528Z",
     "shell.execute_reply": "2023-12-20T07:26:43.926091Z"
    },
    "papermill": {
     "duration": 1.094987,
     "end_time": "2023-12-20T07:26:43.930207",
     "exception": false,
     "start_time": "2023-12-20T07:26:42.835220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n",
      "165\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "all_feats = {}\n",
    "all_results = []\n",
    "for (test, revealed_targets, sample_prediction) in iter_test:\n",
    "    date = test['date_id'].unique()[0]\n",
    "    if all_feats.get(date,0) == 0:\n",
    "        all_feats[date] = []\n",
    "    second = test['seconds_in_bucket'].unique()[0]\n",
    "    currently_scored = test['currently_scored'].unique()[0]\n",
    "    if not currently_scored:  # 不进行调试\n",
    "        sample_prediction['target'] = 0.0\n",
    "        env.predict(sample_prediction)\n",
    "        counter += 1\n",
    "        continue\n",
    "    print(date, second, currently_scored)\n",
    "    n_test = test.set_index(['seconds_in_bucket', 'stock_id'])\n",
    "    del n_test['date_id'], n_test['row_id'], n_test['currently_scored']\n",
    "    # raw features\n",
    "    if second == 0:\n",
    "        features = generate_features(n_test)\n",
    "        del features['imbalance_buy_sell_flag']\n",
    "        feat = features.copy()\n",
    "    else:\n",
    "        feat = generate_features(n_test)\n",
    "        del feat['imbalance_buy_sell_flag']\n",
    "        features = pd.concat([features, feat], axis=0)\n",
    "    # expand features\n",
    "    tmp_features = []\n",
    "    for f in features.columns:\n",
    "        fac_to_remain=feature_lst0 if second<300 else feature_lst01\n",
    "        tmp = cal_extra_fea(df_original_fea=features[f], \n",
    "                            fac_to_remain=fac_to_remain, \n",
    "                            SUB_MODE=True)\n",
    "        tmp_features.append(tmp)\n",
    "    tmp_features = pd.concat(tmp_features, axis=1)\n",
    "    tmp_features = tmp_features.reset_index()\n",
    "    tmp_features['seconds_in_bucket'] = second\n",
    "    tmp_features = tmp_features.set_index(['seconds_in_bucket', 'stock_id'])\n",
    "    # market factors\n",
    "    market_feats = market_features(feat)\n",
    "    # merge factors\n",
    "    feat = feat.merge(right=market_feats, \n",
    "                      left_on='seconds_in_bucket', \n",
    "                      right_index=True, \n",
    "                      how='left')\n",
    "    feat = pd.concat([feat, tmp_features], axis=1)\n",
    "    feat = feat.replace([np.inf, -np.inf], np.nan)\n",
    "    all_feats[date].append(feat)\n",
    "    # predict\n",
    "    if second < 80:\n",
    "        tree_predictions_0 = ensemble_models.ensemble_tree_predict(feat, 0)\n",
    "        sample_prediction['target'] = tree_predictions_0\n",
    "    elif second < 300:\n",
    "        tree_predictions_0 = ensemble_models.ensemble_tree_predict(feat, 0)\n",
    "        nn_predictions_0 = ensemble_models.ensemble_nn_predict(all_feats, second, 0)\n",
    "#         print(nn_predictions_0.shape)\n",
    "        sample_prediction['target'] = (tree_predictions_0+nn_predictions_0)/2\n",
    "#         sample_prediction['target'] = nn_predictions_0\n",
    "    elif second < 380:\n",
    "        tree_predictions_0 = ensemble_models.ensemble_tree_predict(feat, 0)\n",
    "        tree_predictions_1 = ensemble_models.ensemble_tree_predict(feat, 1)\n",
    "        sample_prediction['target'] = tree_predictions_0+tree_predictions_1\n",
    "    else:\n",
    "        tree_predictions_0 = ensemble_models.ensemble_tree_predict(feat, 0)\n",
    "        tree_predictions_1 = ensemble_models.ensemble_tree_predict(feat, 1)\n",
    "        nn_predictions_0 = ensemble_models.ensemble_nn_predict(all_feats, second, 0)\n",
    "#         nn_predictions_1 = ensemble_models.ensemble_nn_predict(all_feats, second, 1)\n",
    "        predictions_0 = (tree_predictions_0+nn_predictions_0)/2\n",
    "#         predictions_1 = (tree_predictions_1+nn_predictions_1)/2\n",
    "#         predictions_0 = nn_predictions_0\n",
    "        predictions_1 = tree_predictions_1\n",
    "        sample_prediction['target'] = predictions_0+predictions_1\n",
    "    env.predict(sample_prediction)\n",
    "    counter += 1\n",
    "print(counter)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7056235,
     "sourceId": 57891,
     "sourceType": "competition"
    },
    {
     "datasetId": 4152981,
     "sourceId": 7239298,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30615,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 86.645013,
   "end_time": "2023-12-20T07:26:46.173897",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-12-20T07:25:19.528884",
   "version": "2.4.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
